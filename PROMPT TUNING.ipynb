{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e144eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "INLEGALBERT_MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "TRAIN_PATH = \"build_jsonl/build_train.jsonl\"\n",
    "DEV_PATH = \"build_jsonl/build_dev.jsonl\"\n",
    "TEST_PATH = \"build_jsonl/build_test.jsonl\"\n",
    "\n",
    "OUT_DIR = \"prompt_tuning_enhanced_macro_f1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "MAX_SEQ_LENGTH = 128\n",
    "MAX_SENTS_PER_DOC = 64\n",
    "BATCH_DOCS = 4\n",
    "\n",
    "# ==================== PROMPT CONFIGURATION ====================\n",
    "# Hard Prompt (prepended as text - provides legal context)\n",
    "HARD_LEGAL_PROMPT = \"\"\"Indian Legal Judgment Rhetorical Role Classification System:\n",
    "Task: Classify each sentence into one of 13 legal rhetorical roles.\n",
    "Classes: PREAMBLE (document introduction), FAC (facts of the case), \n",
    "RLC (rulings by lower court), ISSUE (key legal issues), \n",
    "ARG_PETITIONER (petitioner arguments), ARG_RESPONDENT (respondent arguments),\n",
    "ANALYSIS (court's reasoning), STA (statutes cited), \n",
    "PRE_RELIED (precedents followed), PRE_NOT_RELIED (precedents distinguished),\n",
    "RATIO (legal principles established), RPC (rulings by present court), NONE (other).\n",
    "Context: Indian Supreme Court judgments follow specific rhetorical structure.\n",
    "Sentence to classify:\"\"\"\n",
    "\n",
    "# Soft Prompt Configuration\n",
    "NUM_SOFT_PROMPT_TOKENS = 35  # Learnable continuous embeddings\n",
    "CLASS_PROMPT_TOKENS = 20     # Per-class specific prompts\n",
    "CONTEXT_PROMPT_TOKENS = 10   # Context-adaptive prompts\n",
    "\n",
    "# ==================== TRAINING HYPERPARAMETERS ====================\n",
    "NUM_EPOCHS = 20\n",
    "PROMPT_LR = 8e-4      # High LR for prompt parameters\n",
    "CLASSIFIER_LR = 3e-5  # Low LR for classifier layers\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_RATIO = 0.15\n",
    "\n",
    "# Loss Configuration\n",
    "FOCAL_GAMMA = 2.5\n",
    "FOCAL_ALPHA = 0.25\n",
    "LABEL_SMOOTHING = 0.08\n",
    "PROTO_WEIGHT = 0.18\n",
    "RPL_WEIGHT = 0.08\n",
    "RTM_LAMBDA = 0.03\n",
    "PROTO_AUX_TEMPERATURE = 5.0\n",
    "\n",
    "# Minority Class Strategy\n",
    "MINORITY_BOOST = 6.0  # Aggressive boosting for macro F1\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "USE_POSITIONAL_EMB = True\n",
    "POS_EMB_DIM = 32\n",
    "USE_KNN_PRIOR = True\n",
    "KNN_K = 3\n",
    "KNN_PRIOR_DIM = 64\n",
    "\n",
    "# Architecture\n",
    "LSTM_HIDDEN = 512\n",
    "DROPOUT = 0.35\n",
    "\n",
    "LABELS = [\n",
    "    \"PREAMBLE\", \"FAC\", \"RLC\", \"ISSUE\", \"ARG_PETITIONER\",\n",
    "    \"ARG_RESPONDENT\", \"ANALYSIS\", \"STA\", \"PRE_RELIED\",\n",
    "    \"PRE_NOT_RELIED\", \"RATIO\", \"RPC\", \"NONE\",\n",
    "]\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(LABELS)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "NUM_LABELS = len(LABELS)\n",
    "\n",
    "MINORITY_CLASSES = [\"RLC\", \"ISSUE\", \"STA\", \"RATIO\", \"PRE_RELIED\", \"PRE_NOT_RELIED\", \"RPC\"]\n",
    "minority_ids = [label2id[label] for label in MINORITY_CLASSES if label in label2id]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ ADVANCED PROMPT TUNING FOR LEGAL CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Hard Prompt: {len(HARD_LEGAL_PROMPT.split())} words\")\n",
    "print(f\"Soft Prompt Tokens: {NUM_SOFT_PROMPT_TOKENS}\")\n",
    "print(f\"Class-Specific Prompts: {CLASS_PROMPT_TOKENS} tokens Ã— {NUM_LABELS} classes\")\n",
    "print(f\"Minority Boost Factor: {MINORITY_BOOST}x\")\n",
    "print(f\"Target: Macro F1 > 0.59\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== UTILITIES ====================\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def load_jsonl(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                data.append(json.loads(s))\n",
    "    return data\n",
    "\n",
    "def extract_data(docs, max_sents=MAX_SENTS_PER_DOC):\n",
    "    all_sents, all_labels, doc_ids = [], [], []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.get(\"id\", \"\")\n",
    "        sents, labs = [], []\n",
    "        \n",
    "        if \"sentences\" in doc and \"labels\" in doc:\n",
    "            sents = doc[\"sentences\"]\n",
    "            labs = [label2id.get(l, label2id[\"NONE\"]) for l in doc[\"labels\"]]\n",
    "        elif \"sentences\" in doc and \"annotation\" in doc:\n",
    "            sents = doc[\"sentences\"]\n",
    "            labs = [label2id.get(l, label2id[\"NONE\"]) for l in doc[\"annotation\"]]\n",
    "        elif \"annotations\" in doc:\n",
    "            for a in doc.get(\"annotations\", []):\n",
    "                for item in a.get(\"result\", []):\n",
    "                    val = item.get(\"value\", {})\n",
    "                    text = val.get(\"text\", \"\").strip()\n",
    "                    labs_list = val.get(\"labels\", [\"NONE\"])\n",
    "                    if text:\n",
    "                        sents.append(text)\n",
    "                        labs.append(label2id.get(labs_list[0], label2id[\"NONE\"]))\n",
    "        \n",
    "        if len(sents) > max_sents:\n",
    "            sents = sents[:max_sents]\n",
    "            labs = labs[:max_sents]\n",
    "        \n",
    "        if sents and labs and len(sents) == len(labs):\n",
    "            all_sents.append(sents)\n",
    "            all_labels.append(labs)\n",
    "            doc_ids.append(doc_id)\n",
    "    \n",
    "    return all_sents, all_labels, doc_ids\n",
    "\n",
    "def get_memory_usage(device=DEVICE):\n",
    "    if torch.cuda.is_available() and device.type == 'cuda':\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated(device) / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved(device) / 1024**2,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "        }\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        return {'cpu_ram_mb': process.memory_info().rss / 1024**2}\n",
    "\n",
    "def compute_detailed_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0, labels=range(NUM_LABELS)\n",
    "    )\n",
    "    \n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Minority class metrics\n",
    "    minority_mask = np.isin(y_true, minority_ids)\n",
    "    if minority_mask.sum() > 0:\n",
    "        minority_true = y_true[minority_mask]\n",
    "        minority_pred = y_pred[minority_mask]\n",
    "        minority_f1 = f1_score(minority_true, minority_pred, average='macro', zero_division=0)\n",
    "    else:\n",
    "        minority_f1 = 0.0\n",
    "    \n",
    "    minority_f1_per_class = {}\n",
    "    for cls_id in minority_ids:\n",
    "        mask = (y_true == cls_id)\n",
    "        if mask.sum() > 1:\n",
    "            try:\n",
    "                minority_f1_per_class[id2label[cls_id]] = f1_score(\n",
    "                    y_true[mask], y_pred[mask], average='binary', zero_division=0\n",
    "                )\n",
    "            except:\n",
    "                minority_f1_per_class[id2label[cls_id]] = 0.0\n",
    "        else:\n",
    "            minority_f1_per_class[id2label[cls_id]] = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'weighted_f1': float(weighted_f1),\n",
    "        'macro_precision': float(np.mean(precision)),\n",
    "        'macro_recall': float(np.mean(recall)),\n",
    "        'minority_macro_f1': float(minority_f1),\n",
    "        'minority_f1_per_class': minority_f1_per_class,\n",
    "        'per_class_precision': {id2label[i]: float(precision[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_recall': {id2label[i]: float(recall[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_f1': {id2label[i]: float(f1[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_support': {id2label[i]: int(support[i]) for i in range(NUM_LABELS)},\n",
    "    }\n",
    "\n",
    "# ==================== PROMPT TUNING MODULE ====================\n",
    "class AdvancedPromptTuning(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Level Prompt Tuning:\n",
    "    1. Soft Prompts: Learnable continuous embeddings\n",
    "    2. Class-Specific Prompts: Per-class trainable prompts\n",
    "    3. Context-Adaptive Selection: Dynamic prompt weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_soft_tokens=NUM_SOFT_PROMPT_TOKENS,\n",
    "                 class_tokens=CLASS_PROMPT_TOKENS, context_tokens=CONTEXT_PROMPT_TOKENS):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_soft_tokens = num_soft_tokens\n",
    "        self.class_tokens = class_tokens\n",
    "        \n",
    "        # 1. Global Soft Prompts (shared across all inputs)\n",
    "        # Initialize with small random values for stability\n",
    "        self.global_soft_prompt = nn.Parameter(\n",
    "            torch.randn(1, num_soft_tokens, hidden_size) * 0.02\n",
    "        )\n",
    "        \n",
    "        # 2. Class-Specific Soft Prompts (one per class)\n",
    "        self.class_specific_prompts = nn.Parameter(\n",
    "            torch.randn(NUM_LABELS, class_tokens, hidden_size) * 0.02\n",
    "        )\n",
    "        \n",
    "        # 3. Context-Adaptive Prompt Selector\n",
    "        # Learns to weight different prompts based on input\n",
    "        self.prompt_selector = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, NUM_LABELS),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # 4. Prompt Fusion Layer\n",
    "        self.prompt_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        \n",
    "        # 5. Gating mechanism for prompt influence\n",
    "        self.prompt_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Prompt Tuning Module Initialized:\")\n",
    "        print(f\"   - Global Soft Prompts: {num_soft_tokens} tokens\")\n",
    "        print(f\"   - Class-Specific Prompts: {class_tokens} Ã— {NUM_LABELS} classes\")\n",
    "        print(f\"   - Total Prompt Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, sent_emb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sent_emb: [B, S, H] sentence embeddings\n",
    "        Returns:\n",
    "            enhanced_emb: [B, S, H] prompt-enhanced embeddings\n",
    "        \"\"\"\n",
    "        B, S, H = sent_emb.shape\n",
    "        \n",
    "        # 1. Apply Global Soft Prompts\n",
    "        # Expand to batch size and concatenate with sentence embeddings\n",
    "        global_prompt = self.global_soft_prompt.expand(B, -1, -1)  # [B, num_soft_tokens, H]\n",
    "        \n",
    "        # 2. Compute Class-Specific Prompt Weights\n",
    "        # Use mean pooled sentence representation for context\n",
    "        sent_context = sent_emb.mean(dim=1)  # [B, H]\n",
    "        class_weights = self.prompt_selector(sent_context)  # [B, NUM_LABELS]\n",
    "        \n",
    "        # 3. Weighted Combination of Class-Specific Prompts\n",
    "        # Blend class prompts based on predicted relevance\n",
    "        class_prompts = torch.einsum('bc,cth->bth', \n",
    "                                     class_weights, \n",
    "                                     self.class_specific_prompts)  # [B, class_tokens, H]\n",
    "        \n",
    "        # 4. Combine Global and Class Prompts\n",
    "        combined_prompts = torch.cat([global_prompt, class_prompts], dim=1)  # [B, num_soft_tokens+class_tokens, H]\n",
    "        \n",
    "        # 5. Attend from Sentences to Prompts\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.bmm(sent_emb, combined_prompts.transpose(1, 2))  # [B, S, num_soft_tokens+class_tokens]\n",
    "        attn_weights = F.softmax(attn_scores / np.sqrt(H), dim=-1)\n",
    "        \n",
    "        # 6. Get Prompt Context\n",
    "        prompt_context = torch.bmm(attn_weights, combined_prompts)  # [B, S, H]\n",
    "        \n",
    "        # 7. Fuse Original and Prompt Features\n",
    "        fused = torch.cat([sent_emb, prompt_context], dim=-1)  # [B, S, 2H]\n",
    "        fused = self.prompt_fusion(fused)  # [B, S, H]\n",
    "        \n",
    "        # 8. Gated Addition (learnable skip connection)\n",
    "        gate = self.prompt_gate(sent_emb)\n",
    "        enhanced_emb = gate * fused + (1 - gate) * sent_emb\n",
    "        \n",
    "        return enhanced_emb\n",
    "\n",
    "# ==================== PROTOTYPE MANAGER ====================\n",
    "class ClassPrototypeManager:\n",
    "    \"\"\"Manages class prototypes for prototypical learning\"\"\"\n",
    "    def __init__(self):\n",
    "        self.prototypes = None\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, embeddings, labels):\n",
    "        embeddings = np.asarray(embeddings)\n",
    "        labels = np.asarray(labels)\n",
    "        D = embeddings.shape[1]\n",
    "        protos = np.zeros((NUM_LABELS, D), dtype=np.float32)\n",
    "        \n",
    "        for k in range(NUM_LABELS):\n",
    "            mask = labels == k\n",
    "            if mask.sum() > 0:\n",
    "                protos[k] = embeddings[mask].mean(axis=0)\n",
    "            else:\n",
    "                protos[k] = np.random.randn(D).astype(np.float32) * 1e-3\n",
    "        \n",
    "        self.prototypes = protos\n",
    "        self.fitted = True\n",
    "        print(f\"[Prototypes] Fitted {NUM_LABELS} class prototypes, shape: {protos.shape}\")\n",
    "\n",
    "    def get_all_tensor(self, device=None):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        t = torch.tensor(self.prototypes, dtype=torch.float32)\n",
    "        if device is not None:\n",
    "            t = t.to(device)\n",
    "        return t\n",
    "\n",
    "    def get_nearest_index(self, embeddings):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        emb = np.asarray(embeddings)\n",
    "        emb_n = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-8)\n",
    "        proto_n = self.prototypes / (np.linalg.norm(self.prototypes, axis=1, keepdims=True) + 1e-8)\n",
    "        sims = emb_n @ proto_n.T\n",
    "        return np.argmax(sims, axis=1)\n",
    "\n",
    "    def knn_prior(self, embeddings, topk=KNN_K):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        emb = np.asarray(embeddings)\n",
    "        emb_n = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
    "        proto_n = self.prototypes / (np.linalg.norm(self.prototypes, axis=1, keepdims=True) + 1e-12)\n",
    "        sims = emb_n @ proto_n.T\n",
    "        topk_idx = np.argsort(-sims, axis=1)[:, :topk]\n",
    "        topk_sims = np.take_along_axis(sims, topk_idx, axis=1)\n",
    "        return topk_sims, topk_idx\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "class DocumentTextDataset(Dataset):\n",
    "    def __init__(self, docs_sents, docs_labels):\n",
    "        self.docs_sents = docs_sents\n",
    "        self.docs_labels = docs_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs_sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sentences\": self.docs_sents[idx],\n",
    "            \"labels\": torch.tensor(self.docs_labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def collate_docs(batch, tokenizer):\n",
    "    max_sents = max(len(b[\"sentences\"]) for b in batch)\n",
    "    B = len(batch)\n",
    "\n",
    "    flat_sents = []\n",
    "    doc_sent_offsets = []\n",
    "    for b in batch:\n",
    "        doc_sent_offsets.append(len(flat_sents))\n",
    "        flat_sents.extend(b[\"sentences\"])\n",
    "\n",
    "    enc = tokenizer(\n",
    "        flat_sents,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids_all = enc[\"input_ids\"]\n",
    "    attn_mask_all = enc[\"attention_mask\"]\n",
    "\n",
    "    max_tokens = input_ids_all.shape[1]\n",
    "    input_ids_padded = torch.zeros((B, max_sents, max_tokens), dtype=torch.long)\n",
    "    attn_mask_padded = torch.zeros((B, max_sents, max_tokens), dtype=torch.long)\n",
    "    labels_padded = torch.full((B, max_sents), -100, dtype=torch.long)\n",
    "    lengths = torch.zeros(B, dtype=torch.long)\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        num_s = len(b[\"sentences\"])\n",
    "        lengths[i] = num_s\n",
    "        start = doc_sent_offsets[i]\n",
    "        end = start + num_s\n",
    "        input_ids_padded[i, :num_s] = input_ids_all[start:end]\n",
    "        attn_mask_padded[i, :num_s] = attn_mask_all[start:end]\n",
    "        labels_padded[i, :num_s] = b[\"labels\"]\n",
    "\n",
    "    return input_ids_padded, attn_mask_padded, labels_padded, lengths\n",
    "\n",
    "# ==================== LOSS FUNCTIONS ====================\n",
    "def focal_loss(logits_masked, labels_masked, gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA, label_smoothing=LABEL_SMOOTHING):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    ce = F.cross_entropy(logits_masked, labels_masked, reduction='none', label_smoothing=label_smoothing)\n",
    "    pt = torch.exp(-ce)\n",
    "    focal = alpha * (1-pt)**gamma * ce\n",
    "    return focal.mean()\n",
    "\n",
    "def prototypical_cosine_loss(reprs, prototypes_tensor, labels, temperature=PROTO_AUX_TEMPERATURE):\n",
    "    \"\"\"Prototypical loss using cosine similarity\"\"\"\n",
    "    x_norm = F.normalize(reprs, p=2, dim=1)\n",
    "    p_norm = F.normalize(prototypes_tensor, p=2, dim=1)\n",
    "    sims = x_norm @ p_norm.t()\n",
    "    sims = sims * temperature\n",
    "    loss = F.cross_entropy(sims, labels)\n",
    "    return loss, sims\n",
    "\n",
    "def compute_losses(logits, labels, sent_emb_flat, doc_out, prototypes_tensor, rpl_proto, temperature=PROTO_AUX_TEMPERATURE):\n",
    "    \"\"\"Compute combined losses\"\"\"\n",
    "    logits_flat = logits.view(-1, NUM_LABELS)\n",
    "    labels_flat = labels.view(-1)\n",
    "    mask = labels_flat != -100\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device), torch.tensor(0.0, device=logits.device), torch.tensor(0.0, device=logits.device)\n",
    "    \n",
    "    logits_masked = logits_flat[mask]\n",
    "    labels_masked = labels_flat[mask]\n",
    "    \n",
    "    # 1. Focal Cross-Entropy Loss\n",
    "    ce_loss = focal_loss(logits_masked, labels_masked)\n",
    "    \n",
    "    # 2. Prototypical Loss\n",
    "    valid_sent_emb = sent_emb_flat[mask]\n",
    "    proto_loss, _ = prototypical_cosine_loss(valid_sent_emb, prototypes_tensor, labels_masked)\n",
    "    \n",
    "    # 3. Role Prototypical Layer Loss\n",
    "    valid_doc_out = doc_out.view(-1, doc_out.size(-1))[mask]\n",
    "    rpl_sim = F.normalize(valid_doc_out, dim=1) @ F.normalize(rpl_proto, dim=1).T\n",
    "    rpl_loss = F.cross_entropy(rpl_sim * temperature, labels_masked)\n",
    "    \n",
    "    return ce_loss, proto_loss, rpl_loss\n",
    "\n",
    "# ==================== MODEL COMPONENTS ====================\n",
    "class RolePrototypicalLayer(nn.Module):\n",
    "    \"\"\"Role-specific prototypical classification head\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.proto = nn.Parameter(torch.randn(NUM_LABELS, hidden_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        h_norm = F.normalize(h, dim=-1)\n",
    "        proto_norm = F.normalize(self.proto, dim=-1)\n",
    "        return h_norm @ proto_norm.T\n",
    "\n",
    "class RoleTransitionMatrix(nn.Module):\n",
    "    \"\"\"Models sequential dependencies between rhetorical roles\"\"\"\n",
    "    def __init__(self, rtm_lambda=RTM_LAMBDA):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.zeros(NUM_LABELS, NUM_LABELS))\n",
    "        self.rtm_lambda = rtm_lambda\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        lp = logits.log_softmax(-1)\n",
    "        B, S, C = lp.shape\n",
    "        \n",
    "        for t in range(1, S):\n",
    "            tr = torch.logsumexp(\n",
    "                lp[:, t-1].unsqueeze(2) + self.A.log_softmax(-1),\n",
    "                dim=1\n",
    "            )\n",
    "            logits[:, t] += self.rtm_lambda * tr\n",
    "        return logits\n",
    "\n",
    "class SentenceEncoderFFN(nn.Module):\n",
    "    \"\"\"Feed-forward network for sentence encoding\"\"\"\n",
    "    def __init__(self, sent_dim, hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(sent_dim, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, sent_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(sent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = self.act(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "class PrototypeAttention(nn.Module):\n",
    "    \"\"\"Attention mechanism over class prototypes\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, sent_emb, prototypes):\n",
    "        B, S, H = sent_emb.shape\n",
    "        h_proj = self.W(sent_emb)\n",
    "        scores = torch.matmul(h_proj, prototypes.t())\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        proto_ctx = torch.matmul(attn_weights, prototypes)\n",
    "        return proto_ctx, attn_weights\n",
    "\n",
    "# ==================== MAIN MODEL ====================\n",
    "class PromptTuningHSLNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Prompt Tuning + Hierarchical Sentence-Level Network\n",
    "    with Role Prototypical Layer and Transition Modeling\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_name=INLEGALBERT_MODEL_NAME, pos_dim=POS_EMB_DIM,\n",
    "                 use_pos_emb=USE_POSITIONAL_EMB, use_knn_prior=USE_KNN_PRIOR,\n",
    "                 knn_prior_dim=KNN_PRIOR_DIM, doc_hidden=LSTM_HIDDEN, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        # âœ… FROZEN BERT (NO LoRA/PEFT)\n",
    "        self.bert = AutoModel.from_pretrained(bert_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(\"\\nâœ… BERT Model: COMPLETELY FROZEN (no gradients)\")\n",
    "        print(\"âœ… Training Strategy: PURE PROMPT TUNING\")\n",
    "\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # âœ… ADVANCED PROMPT TUNING MODULE\n",
    "        self.prompt_tuning = AdvancedPromptTuning(self.hidden_size)\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        self.use_pos_emb = use_pos_emb\n",
    "        if use_pos_emb and pos_dim > 0:\n",
    "            self.pos_emb = nn.Embedding(1024, pos_dim)\n",
    "            self.pos_proj = nn.Linear(pos_dim, self.hidden_size) if pos_dim != self.hidden_size else None\n",
    "        else:\n",
    "            self.pos_emb = None\n",
    "            self.pos_proj = None\n",
    "\n",
    "        # KNN Prior Features\n",
    "        self.use_knn_prior = use_knn_prior\n",
    "        self.knn_prior_dim = knn_prior_dim\n",
    "        if use_knn_prior and knn_prior_dim > 0:\n",
    "            self.knn_proj = nn.Sequential(\n",
    "                nn.Linear(KNN_K, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(64, knn_prior_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.knn_proj = None\n",
    "\n",
    "        # Sentence Encoder\n",
    "        self.sent_encoder = SentenceEncoderFFN(self.hidden_size, hidden=512, dropout=dropout)\n",
    "        \n",
    "        # Prototype Attention\n",
    "        self.proto_attn = PrototypeAttention(self.hidden_size)\n",
    "        \n",
    "        # Document-Level LSTM\n",
    "        final_in_dim = self.hidden_size + (self.knn_prior_dim if self.knn_proj else 0)\n",
    "        self.doc_lstm = nn.LSTM(final_in_dim, doc_hidden, 2, bidirectional=True, \n",
    "                               batch_first=True, dropout=dropout)\n",
    "        \n",
    "        lstm_out_dim = doc_hidden * 2\n",
    "        \n",
    "        # Classification Heads\n",
    "        self.ce_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_out_dim, doc_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(doc_hidden, NUM_LABELS),\n",
    "        )\n",
    "        \n",
    "        self.rpl = RolePrototypicalLayer(lstm_out_dim)\n",
    "        self.rtm = RoleTransitionMatrix()\n",
    "        self.head_alpha = nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    def encode_sentences(self, input_ids, attention_mask):\n",
    "        \"\"\"Encode sentences using frozen BERT\"\"\"\n",
    "        B, S, T = input_ids.shape\n",
    "        input_ids_flat = input_ids.view(B * S, T)\n",
    "        attn_flat = attention_mask.view(B * S, T)\n",
    "        \n",
    "        with torch.no_grad():  # BERT is frozen\n",
    "            outputs = self.bert(input_ids_flat, attention_mask=attn_flat)\n",
    "        \n",
    "        sent_emb_flat = outputs.last_hidden_state.mean(dim=1)\n",
    "        sent_emb = sent_emb_flat.view(B, S, -1)\n",
    "        return sent_emb, sent_emb_flat\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, lengths, prototypes_tensor, \n",
    "                proto_idx_batch=None, knn_sims=None):\n",
    "        B, S, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # 1. Encode Sentences (frozen BERT)\n",
    "        sent_emb, sent_emb_flat = self.encode_sentences(input_ids, attention_mask)\n",
    "        \n",
    "        # 2. âœ… APPLY PROMPT TUNING (trainable)\n",
    "        sent_emb = self.prompt_tuning(sent_emb)\n",
    "        sent_emb = self.sent_encoder(sent_emb)\n",
    "\n",
    "        # 3. Add Positional Embeddings\n",
    "        if self.pos_emb is not None:\n",
    "            pos_idx = torch.arange(S, device=device).unsqueeze(0).expand(B, -1)\n",
    "            pos_vec = self.pos_emb(pos_idx)\n",
    "            if self.pos_proj is not None:\n",
    "                pos_vec = self.pos_proj(pos_vec)\n",
    "            sent_emb = sent_emb + pos_vec\n",
    "\n",
    "        # 4. Prototype Attention\n",
    "        protos = prototypes_tensor if isinstance(prototypes_tensor, torch.Tensor) else \\\n",
    "                torch.tensor(prototypes_tensor, device=device, dtype=torch.float32)\n",
    "        proto_ctx, _ = self.proto_attn(sent_emb, protos)\n",
    "        sent_emb = sent_emb + proto_ctx\n",
    "\n",
    "        # 5. Add KNN Prior Features\n",
    "        if self.knn_proj is not None and knn_sims is not None:\n",
    "            knn_feat = self.knn_proj(knn_sims.view(-1, KNN_K)).view(B, S, -1)\n",
    "            doc_in = torch.cat([sent_emb, knn_feat], dim=2)\n",
    "        else:\n",
    "            doc_in = sent_emb\n",
    "\n",
    "        # 6. Document-Level LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(doc_in, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.doc_lstm(packed)\n",
    "        doc_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        # 7. Multi-Head Classification\n",
    "        ce_logits = self.ce_classifier(doc_out)\n",
    "        rpl_logits = self.rpl(doc_out)\n",
    "        alpha = torch.sigmoid(self.head_alpha)\n",
    "        blended_logits = alpha * ce_logits + (1 - alpha) * rpl_logits\n",
    "        \n",
    "        # 8. Role Transition Modeling\n",
    "        final_logits = self.rtm(blended_logits)\n",
    "        \n",
    "        return final_logits, sent_emb_flat, doc_out\n",
    "\n",
    "# ==================== TRAINER ====================\n",
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer, prototype_manager, device=DEVICE):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prototype_manager = prototype_manager\n",
    "        self.device = device\n",
    "\n",
    "    def _build_train_loader(self, train_dataset):\n",
    "        \"\"\"Build training data loader with weighted sampling for minority classes\"\"\"\n",
    "        if not USE_WEIGHTED_SAMPLER:\n",
    "            return DataLoader(train_dataset, batch_size=BATCH_DOCS, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "        \n",
    "        # Compute document-level majority class\n",
    "        major_labels = []\n",
    "        for doc_labels in train_dataset.docs_labels:\n",
    "            if doc_labels:\n",
    "                doc_counter = Counter(doc_labels)\n",
    "                major_label = doc_counter.most_common(1)[0][0]\n",
    "                major_labels.append(major_label)\n",
    "            else:\n",
    "                major_labels.append(0)\n",
    "        \n",
    "        # Compute inverse frequency weights\n",
    "        counts = np.bincount(major_labels, minlength=NUM_LABELS)\n",
    "        inv_freq = 1.0 / (counts + 1e-6)\n",
    "        \n",
    "        # Apply minority boosting\n",
    "        weights = inv_freq[np.array(major_labels)].copy()\n",
    "        for doc_idx, major_label in enumerate(major_labels):\n",
    "            if major_label in minority_ids:\n",
    "                weights[doc_idx] *= MINORITY_BOOST\n",
    "        \n",
    "        sampler = WeightedRandomSampler(\n",
    "            torch.tensor(weights, dtype=torch.double),\n",
    "            num_samples=len(weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        return DataLoader(train_dataset, batch_size=BATCH_DOCS, sampler=sampler,\n",
    "                         collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "\n",
    "    def train(self, train_dataset, dev_dataset, num_epochs=NUM_EPOCHS, \n",
    "              proto_weight=PROTO_WEIGHT, rpl_weight=RPL_WEIGHT):\n",
    "        \"\"\"Train the model with prompt tuning\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # âœ… SEPARATE OPTIMIZERS: High LR for prompts, low for classifiers\n",
    "        prompt_params = list(self.model.prompt_tuning.parameters())\n",
    "        other_params = [p for n, p in self.model.named_parameters() \n",
    "                       if p.requires_grad and 'prompt_tuning' not in n]\n",
    "        \n",
    "        prompt_optimizer = torch.optim.AdamW(prompt_params, lr=PROMPT_LR, weight_decay=WEIGHT_DECAY)\n",
    "        other_optimizer = torch.optim.AdamW(other_params, lr=CLASSIFIER_LR, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        train_loader = self._build_train_loader(train_dataset)\n",
    "        total_steps = max(1, len(train_loader) * num_epochs)\n",
    "        warmup_steps = max(1, int(WARMUP_RATIO * total_steps))\n",
    "        \n",
    "        prompt_scheduler = get_linear_schedule_with_warmup(prompt_optimizer, warmup_steps, total_steps)\n",
    "        other_scheduler = get_linear_schedule_with_warmup(other_optimizer, warmup_steps, total_steps)\n",
    "\n",
    "        prototypes_tensor = self.prototype_manager.get_all_tensor(device=self.device)\n",
    "        best_macro_f1 = -1.0\n",
    "        best_ckpt = None\n",
    "        history = []\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸš€ STARTING PROMPT TUNING TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Training Strategy:\")\n",
    "        print(f\"  - Prompt LR: {PROMPT_LR} (aggressive)\")\n",
    "        print(f\"  - Classifier LR: {CLASSIFIER_LR} (conservative)\")\n",
    "        print(f\"  - Minority Boost: {MINORITY_BOOST}x\")\n",
    "        print(f\"  - Focal Loss Gamma: {FOCAL_GAMMA}\")\n",
    "        print(f\"  - Total Epochs: {num_epochs}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            self.model.train()\n",
    "            t0 = time.time()\n",
    "            running_ce = running_proto = running_rpl = running_total = 0.0\n",
    "            n_samples = 0\n",
    "\n",
    "            for batch_idx, (input_ids, attn_mask, labels, lengths) in enumerate(train_loader):\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                attn_mask = attn_mask.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                lengths = lengths.to(self.device)\n",
    "\n",
    "                # Compute KNN features (BERT frozen during this)\n",
    "                with torch.no_grad():\n",
    "                    sent_emb, sent_emb_flat_for_knn = self.model.encode_sentences(input_ids, attn_mask)\n",
    "                    sent_emb_flat_for_knn_np = sent_emb_flat_for_knn.cpu().numpy()\n",
    "                    sims, _ = self.prototype_manager.knn_prior(sent_emb_flat_for_knn_np, topk=KNN_K)\n",
    "                    knn_sims_tensor = torch.tensor(sims, dtype=torch.float32, device=self.device)\n",
    "                    nearest_idx_flat = self.prototype_manager.get_nearest_index(sent_emb_flat_for_knn_np)\n",
    "                    proto_idx_batch = torch.tensor(nearest_idx_flat, dtype=torch.long, \n",
    "                                                 device=self.device).view(input_ids.shape[:2])\n",
    "\n",
    "                # Forward pass\n",
    "                logits, sent_emb_flat, doc_out = self.model(\n",
    "                    input_ids, attn_mask, lengths, prototypes_tensor,\n",
    "                    proto_idx_batch, knn_sims_tensor\n",
    "                )\n",
    "\n",
    "                # Compute losses\n",
    "                ce_loss, proto_loss, rpl_loss = compute_losses(\n",
    "                    logits, labels, sent_emb_flat, doc_out,\n",
    "                    prototypes_tensor, self.model.rpl.proto\n",
    "                )\n",
    "                total_loss = ce_loss + proto_weight * proto_loss + rpl_weight * rpl_loss\n",
    "\n",
    "                # Backward pass with separate optimizers\n",
    "                prompt_optimizer.zero_grad()\n",
    "                other_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), GRAD_CLIP)\n",
    "                prompt_optimizer.step()\n",
    "                other_optimizer.step()\n",
    "                prompt_scheduler.step()\n",
    "                other_scheduler.step()\n",
    "\n",
    "                # Track metrics\n",
    "                mask = labels.view(-1) != -100\n",
    "                n = mask.sum().item()\n",
    "                running_ce += ce_loss.item() * n\n",
    "                running_proto += proto_loss.item() * n\n",
    "                running_rpl += rpl_loss.item() * n\n",
    "                running_total += total_loss.item() * n\n",
    "                n_samples += n\n",
    "\n",
    "            # Validation\n",
    "            val_results = self.evaluate(dev_dataset, measure_time=True)\n",
    "            avg_ce = running_ce / max(1, n_samples)\n",
    "            avg_proto = running_proto / max(1, n_samples)\n",
    "            avg_rpl = running_rpl / max(1, n_samples)\n",
    "            avg_total = running_total / max(1, n_samples)\n",
    "\n",
    "            epoch_time = time.time() - t0\n",
    "            mem_usage = get_memory_usage(self.device)\n",
    "\n",
    "            # Log history\n",
    "            history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_ce\": avg_ce,\n",
    "                \"train_proto\": avg_proto,\n",
    "                \"train_rpl\": avg_rpl,\n",
    "                \"train_total\": avg_total,\n",
    "                \"val_acc\": val_results[\"metrics\"][\"accuracy\"],\n",
    "                \"val_macro_f1\": val_results[\"metrics\"][\"macro_f1\"],\n",
    "                \"val_minority_f1\": val_results[\"metrics\"][\"minority_macro_f1\"],\n",
    "                \"epoch_time_s\": epoch_time,\n",
    "                \"mem_allocated_mb\": mem_usage.get('allocated', 0),\n",
    "                \"total_time_s\": time.time() - start_time,\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch:02d}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {avg_total:.4f} (CE:{avg_ce:.4f} Proto:{avg_proto:.4f} RPL:{avg_rpl:.4f}) | \"\n",
    "                  f\"Val Acc: {val_results['metrics']['accuracy']:.4f} | \"\n",
    "                  f\"Val Macro-F1: {val_results['metrics']['macro_f1']:.4f} | \"\n",
    "                  f\"Val Minority-F1: {val_results['metrics']['minority_macro_f1']:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.1f}s | \"\n",
    "                  f\"Mem: {mem_usage.get('allocated', 0):.0f}MB\")\n",
    "\n",
    "            # Save best checkpoint\n",
    "            if val_results[\"metrics\"][\"macro_f1\"] > best_macro_f1 + 1e-5:\n",
    "                best_macro_f1 = val_results[\"metrics\"][\"macro_f1\"]\n",
    "                ckpt_path = os.path.join(OUT_DIR, f\"best_epoch{epoch}_macroF1_{best_macro_f1:.4f}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"macro_f1\": best_macro_f1,\n",
    "                    \"metrics\": val_results[\"metrics\"],\n",
    "                }, ckpt_path)\n",
    "                if best_ckpt and os.path.exists(best_ckpt):\n",
    "                    os.remove(best_ckpt)\n",
    "                best_ckpt = ckpt_path\n",
    "                print(f\"  ðŸ’¾ Saved new best checkpoint: {ckpt_path}\")\n",
    "\n",
    "        total_train_time = time.time() - start_time\n",
    "        \n",
    "        # Save training history\n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_df.to_csv(os.path.join(OUT_DIR, \"training_history.csv\"), index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"âœ… TRAINING COMPLETED\")\n",
    "        print(f\"   Total Time: {total_train_time/60:.2f} minutes\")\n",
    "        print(f\"   Best Macro F1: {best_macro_f1:.4f}\")\n",
    "        print(f\"   Best Checkpoint: {best_ckpt}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return best_ckpt, total_train_time\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, measure_time=False):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.model.eval()\n",
    "        \n",
    "        loader = DataLoader(dataset, batch_size=8, shuffle=False,\n",
    "                           collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "        prototypes_tensor = self.prototype_manager.get_all_tensor(device=self.device)\n",
    "        \n",
    "        all_preds, all_trues = [], []\n",
    "        running_ce = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for input_ids, attn_mask, labels, lengths in loader:\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            attn_mask = attn_mask.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            lengths = lengths.to(self.device)\n",
    "\n",
    "            # KNN features\n",
    "            sent_emb, sent_emb_flat_for_knn = self.model.encode_sentences(input_ids, attn_mask)\n",
    "            sent_emb_flat_for_knn_np = sent_emb_flat_for_knn.cpu().numpy()\n",
    "            sims, _ = self.prototype_manager.knn_prior(sent_emb_flat_for_knn_np, topk=KNN_K)\n",
    "            knn_sims_tensor = torch.tensor(sims, dtype=torch.float32, device=self.device)\n",
    "            nearest_idx_flat = self.prototype_manager.get_nearest_index(sent_emb_flat_for_knn_np)\n",
    "            proto_idx_batch = torch.tensor(nearest_idx_flat, dtype=torch.long, \n",
    "                                         device=self.device).view(input_ids.shape[:2])\n",
    "\n",
    "            # Forward pass\n",
    "            logits, sent_emb_flat, doc_out = self.model(\n",
    "                input_ids, attn_mask, lengths, prototypes_tensor,\n",
    "                proto_idx_batch, knn_sims_tensor\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits_flat = logits.view(-1, NUM_LABELS)\n",
    "            labels_flat = labels.view(-1)\n",
    "            mask = labels_flat != -100\n",
    "            \n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "                \n",
    "            logits_masked = logits_flat[mask]\n",
    "            labels_masked = labels_flat[mask]\n",
    "            \n",
    "            ce_loss = focal_loss(logits_masked, labels_masked)\n",
    "            preds = torch.argmax(logits_masked, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_trues.extend(labels_masked.cpu().numpy().tolist())\n",
    "            \n",
    "            n = labels_masked.size(0)\n",
    "            running_ce += ce_loss.item() * n\n",
    "            n_samples += n\n",
    "\n",
    "        if n_samples == 0:\n",
    "            return {\n",
    "                \"metrics\": {\"accuracy\": 0, \"macro_f1\": 0},\n",
    "                \"total_loss\": 0,\n",
    "                \"inference_time\": 0\n",
    "            }\n",
    "\n",
    "        # Compute metrics\n",
    "        avg_ce = running_ce / n_samples\n",
    "        detailed_metrics = compute_detailed_metrics(all_trues, all_preds)\n",
    "        \n",
    "        cls_report = classification_report(\n",
    "            [id2label[x] for x in all_trues],\n",
    "            [id2label[x] for x in all_preds],\n",
    "            digits=4,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"metrics\": detailed_metrics,\n",
    "            \"total_loss\": avg_ce,\n",
    "            \"classification_report\": cls_report,\n",
    "            \"all_preds\": all_preds,\n",
    "            \"all_trues\": all_trues,\n",
    "            \"sample_count\": n_samples\n",
    "        }\n",
    "        \n",
    "        if measure_time:\n",
    "            result[\"inference_time\"] = time.time() - start_time\n",
    "            result[\"mem_usage\"] = get_memory_usage(self.device)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "def main():\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING DATASETS\")\n",
    "    print(\"=\"*80)\n",
    "    train_docs = load_jsonl(TRAIN_PATH)\n",
    "    dev_docs = load_jsonl(DEV_PATH)\n",
    "    test_docs = load_jsonl(TEST_PATH)\n",
    "    print(f\"âœ… Train: {len(train_docs)} documents\")\n",
    "    print(f\"âœ… Dev: {len(dev_docs)} documents\")\n",
    "    print(f\"âœ… Test: {len(test_docs)} documents\")\n",
    "\n",
    "    train_sents, train_labels, _ = extract_data(train_docs)\n",
    "    dev_sents, dev_labels, _ = extract_data(dev_docs)\n",
    "    test_sents, test_labels, _ = extract_data(test_docs)\n",
    "    \n",
    "    print(f\"âœ… Train: {len(train_sents)} docs, {sum(len(s) for s in train_sents)} sentences\")\n",
    "    print(f\"âœ… Dev: {len(dev_sents)} docs, {sum(len(s) for s in dev_sents)} sentences\")\n",
    "    print(f\"âœ… Test: {len(test_sents)} docs, {sum(len(s) for s in test_sents)} sentences\")\n",
    "\n",
    "    # Class distribution\n",
    "    flat_train_labels = [l for doc in train_labels for l in doc]\n",
    "    label_dist = Counter(flat_train_labels)\n",
    "    print(\"\\nðŸ“Š Training Label Distribution:\")\n",
    "    for label_id, count in sorted(label_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {id2label[label_id]:20s}: {count:6d} ({100*count/len(flat_train_labels):5.2f}%)\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(INLEGALBERT_MODEL_NAME)\n",
    "    \n",
    "    # Compute prototypes using frozen BERT\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPUTING CLASS PROTOTYPES\")\n",
    "    print(\"=\"*80)\n",
    "    temp_bert = AutoModel.from_pretrained(INLEGALBERT_MODEL_NAME).to(DEVICE)\n",
    "    temp_bert.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        flat_train_sents = [s for doc in train_sents for s in doc]\n",
    "        flat_train_labels_np = np.array([l for doc in train_labels for l in doc], dtype=np.int64)\n",
    "        \n",
    "        train_embs = []\n",
    "        batch_size = 64\n",
    "        for i in range(0, len(flat_train_sents), batch_size):\n",
    "            batch = flat_train_sents[i:i+batch_size]\n",
    "            enc = tokenizer(batch, padding=True, truncation=True,\n",
    "                          max_length=MAX_SEQ_LENGTH, return_tensors=\"pt\").to(DEVICE)\n",
    "            out = temp_bert(**enc).last_hidden_state.mean(dim=1)\n",
    "            train_embs.append(out.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size + 1) % 50 == 0:\n",
    "                print(f\"  Processed {i+len(batch)}/{len(flat_train_sents)} sentences...\")\n",
    "        \n",
    "        train_embs = np.vstack(train_embs)\n",
    "    \n",
    "    del temp_bert\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… Embeddings computed\")\n",
    "\n",
    "    # Fit prototypes\n",
    "    proto_mgr = ClassPrototypeManager()\n",
    "    proto_mgr.fit(train_embs, flat_train_labels_np)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = DocumentTextDataset(train_sents, train_labels)\n",
    "    dev_dataset = DocumentTextDataset(dev_sents, dev_labels)\n",
    "    test_dataset = DocumentTextDataset(test_sents, test_labels)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INITIALIZING MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    model = PromptTuningHSLNModel()\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    prompt_params = sum(p.numel() for p in model.prompt_tuning.parameters())\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Model Parameters:\")\n",
    "    print(f\"   Total:     {total_params:,}\")\n",
    "    print(f\"   Trainable: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "    print(f\"   Frozen:    {frozen_params:,} ({100*frozen_params/total_params:.2f}%)\")\n",
    "    print(f\"   Prompts:   {prompt_params:,} ({100*prompt_params/trainable_params:.2f}% of trainable)\")\n",
    "    print(f\"\\nâœ… Prompt Tuning Only - BERT Frozen\")\n",
    "\n",
    "    # Train\n",
    "    trainer = Trainer(model, tokenizer, proto_mgr, device=DEVICE)\n",
    "    best_ckpt, train_time = trainer.train(\n",
    "        train_dataset, dev_dataset,\n",
    "        proto_weight=PROTO_WEIGHT,\n",
    "        rpl_weight=RPL_WEIGHT\n",
    "    )\n",
    "    \n",
    "    # Load best checkpoint\n",
    "    if best_ckpt and os.path.exists(best_ckpt):\n",
    "        print(f\"\\nâœ… Loading best checkpoint: {best_ckpt}\")\n",
    "        ckpt = torch.load(best_ckpt, map_location=DEVICE)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        print(f\"   Macro F1: {ckpt['macro_f1']:.4f}\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    test_results = trainer.evaluate(test_dataset, measure_time=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Test Results:\")\n",
    "    print(f\"   Accuracy:          {test_results['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"   Macro F1:          {test_results['metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"   Weighted F1:       {test_results['metrics']['weighted_f1']:.4f}\")\n",
    "    print(f\"   Macro Precision:   {test_results['metrics']['macro_precision']:.4f}\")\n",
    "    print(f\"   Macro Recall:      {test_results['metrics']['macro_recall']:.4f}\")\n",
    "    print(f\"   Minority Macro F1: {test_results['metrics']['minority_macro_f1']:.4f}\")\n",
    "    print(f\"   Inference Time:    {test_results['inference_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "    print(test_results[\"classification_report\"])\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Minority Class F1 Scores:\")\n",
    "    for cls, f1 in sorted(test_results['metrics']['minority_f1_per_class'].items(),\n",
    "                         key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {cls:20s}: {f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Per-Class F1 Scores:\")\n",
    "    for cls, f1 in sorted(test_results['metrics']['per_class_f1'].items(),\n",
    "                         key=lambda x: x[1], reverse=True):\n",
    "        support = test_results['metrics']['per_class_support'][cls]\n",
    "        print(f\"   {cls:20s}: {f1:.4f} (n={support})\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    cm = confusion_matrix(test_results[\"all_trues\"], test_results[\"all_preds\"],\n",
    "                         labels=range(NUM_LABELS))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[l.replace('_', '\\n') for l in LABELS],\n",
    "                yticklabels=[l.replace('_', '\\n') for l in LABELS],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(f'Prompt Tuning + HSLN + RPL + RTM\\n'\n",
    "              f'Test Macro F1: {test_results[\"metrics\"][\"macro_f1\"]:.4f} | '\n",
    "              f'Minority F1: {test_results[\"metrics\"][\"minority_macro_f1\"]:.4f}\\n'\n",
    "              f'Accuracy: {test_results[\"metrics\"][\"accuracy\"]:.4f}',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nðŸ’¾ Saved confusion matrix to {OUT_DIR}/confusion_matrix.png\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    results_summary = {\n",
    "        'method': 'Advanced_Prompt_Tuning_HSLN',\n",
    "        'hard_prompt_words': len(HARD_LEGAL_PROMPT.split()),\n",
    "        'soft_prompt_tokens': NUM_SOFT_PROMPT_TOKENS,\n",
    "        'class_prompt_tokens': CLASS_PROMPT_TOKENS,\n",
    "        'total_prompt_params': prompt_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'minority_boost': MINORITY_BOOST,\n",
    "        'focal_gamma': FOCAL_GAMMA,\n",
    "        'total_train_time_minutes': train_time / 60,\n",
    "        'test_inference_time_seconds': test_results['inference_time'],\n",
    "        **test_results['metrics']\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame([results_summary])\n",
    "    summary_df.to_csv(os.path.join(OUT_DIR, \"final_results_summary.csv\"), index=False)\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"true_label\": [id2label[x] for x in test_results[\"all_trues\"]],\n",
    "        \"pred_label\": [id2label[x] for x in test_results[\"all_preds\"]],\n",
    "        \"true_id\": test_results[\"all_trues\"],\n",
    "        \"pred_id\": test_results[\"all_preds\"],\n",
    "        \"correct\": [t == p for t, p in zip(test_results[\"all_trues\"], test_results[\"all_preds\"])]\n",
    "    })\n",
    "    pred_df.to_csv(os.path.join(OUT_DIR, \"final_test_predictions.csv\"), index=False)\n",
    "    \n",
    "    # Save detailed per-class metrics\n",
    "    per_class_df = pd.DataFrame({\n",
    "        'class': LABELS,\n",
    "        'precision': [test_results['metrics']['per_class_precision'][l] for l in LABELS],\n",
    "        'recall': [test_results['metrics']['per_class_recall'][l] for l in LABELS],\n",
    "        'f1': [test_results['metrics']['per_class_f1'][l] for l in LABELS],\n",
    "        'support': [test_results['metrics']['per_class_support'][l] for l in LABELS],\n",
    "    })\n",
    "    per_class_df.to_csv(os.path.join(OUT_DIR, \"per_class_metrics.csv\"), index=False)\n",
    "    \n",
    "    # Save prompts information\n",
    "    prompt_info = {\n",
    "        'hard_prompt': HARD_LEGAL_PROMPT,\n",
    "        'num_soft_tokens': NUM_SOFT_PROMPT_TOKENS,\n",
    "        'class_prompt_tokens': CLASS_PROMPT_TOKENS,\n",
    "        'context_prompt_tokens': CONTEXT_PROMPT_TOKENS,\n",
    "        'prompt_lr': PROMPT_LR,\n",
    "        'classifier_lr': CLASSIFIER_LR,\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"prompt_configuration.json\"), 'w') as f:\n",
    "        json.dump(prompt_info, f, indent=2)\n",
    "    \n",
    "    total_runtime = time.time() - total_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS SAVED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Output Directory: {OUT_DIR}/\")\n",
    "    print(f\"  âœ… final_results_summary.csv\")\n",
    "    print(f\"  âœ… final_test_predictions.csv\")\n",
    "    print(f\"  âœ… per_class_metrics.csv\")\n",
    "    print(f\"  âœ… training_history.csv\")\n",
    "    print(f\"  âœ… confusion_matrix.png\")\n",
    "    print(f\"  âœ… prompt_configuration.json\")\n",
    "    print(f\"  âœ… Best model checkpoint\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Runtime:      {total_runtime/60:.2f} minutes\")\n",
    "    print(f\"Training Time:      {train_time/60:.2f} minutes\")\n",
    "    print(f\"Test Macro F1:      {test_results['metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"Test Accuracy:      {test_results['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"Minority Macro F1:  {test_results['metrics']['minority_macro_f1']:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if test_results['metrics']['macro_f1'] >= 0.59:\n",
    "        print(\"ðŸŽ‰ TARGET ACHIEVED: Macro F1 â‰¥ 0.59!\")\n",
    "    else:\n",
    "        print(f\"ðŸ“Š Current Macro F1: {test_results['metrics']['macro_f1']:.4f} (Target: 0.59)\")\n",
    "    \n",
    "    print(\"\\nâœ… PROMPT TUNING EXPERIMENT COMPLETED SUCCESSFULLY!\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-nlp",
   "language": "python",
   "name": "legal-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
