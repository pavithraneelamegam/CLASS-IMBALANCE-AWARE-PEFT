{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "from peft import AdaLoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "INLEGALBERT_MODEL_NAME = \"law-ai/InLegalBERT\"\n",
    "TRAIN_PATH = \"build_jsonl/build_train.jsonl\"\n",
    "DEV_PATH = \"build_jsonl/build_dev.jsonl\"\n",
    "TEST_PATH = \"build_jsonl/build_test.jsonl\"\n",
    "\n",
    "OUT_DIR = \"inlegalbert_AdaLoRA_RPL_RTM_fixed_imbalance\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "MAX_SEQ_LENGTH = 128\n",
    "MAX_SENTS_PER_DOC = 64\n",
    "BATCH_DOCS = 2\n",
    "\n",
    "# ‚úÖ HYPERPARAMETERS\n",
    "NUM_EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "PROTO_WEIGHT = 0.15\n",
    "RPL_WEIGHT = 0.05\n",
    "RTM_LAMBDA = 0.02\n",
    "PROTO_AUX_TEMPERATURE = 5.0\n",
    "DROPOUT = 0.3\n",
    "LSTM_HIDDEN = 384\n",
    "LABEL_SMOOTHING = 0.02\n",
    "\n",
    "# Focal Loss params\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "# AdaLoRA specific params\n",
    "ADALORA_INIT_R = 12\n",
    "ADALORA_TARGET_R = 8\n",
    "ADALORA_TINIT = 200\n",
    "ADALORA_TFINAL = 1500\n",
    "ADALORA_DELTA_T = 100\n",
    "\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "USE_POSITIONAL_EMB = True\n",
    "POS_EMB_DIM = 32\n",
    "USE_KNN_PRIOR = True\n",
    "KNN_K = 3\n",
    "KNN_PRIOR_DIM = 64\n",
    "MINORITY_BOOST = 3.0\n",
    "\n",
    "LABELS = [\n",
    "    \"PREAMBLE\", \"FAC\", \"RLC\", \"ISSUE\", \"ARG_PETITIONER\",\n",
    "    \"ARG_RESPONDENT\", \"ANALYSIS\", \"STA\", \"PRE_RELIED\",\n",
    "    \"PRE_NOT_RELIED\", \"RATIO\", \"RPC\", \"NONE\",\n",
    "]\n",
    "label2id = {label: idx for idx, label in enumerate(LABELS)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "NUM_LABELS = len(LABELS)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MINORITY_CLASSES = [\"RLC\", \"ISSUE\", \"STA\", \"RATIO\", \"PRE_RELIED\", \"PRE_NOT_RELIED\", \"RPC\"]\n",
    "minority_ids = [label2id[label] for label in MINORITY_CLASSES if label in label2id]\n",
    "\n",
    "# ---------------- UTILITIES ----------------\n",
    "def get_memory_usage(device=DEVICE):\n",
    "    if torch.cuda.is_available() and device.type == 'cuda':\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated(device) / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved(device) / 1024**2,\n",
    "            'max_allocated': torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "        }\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        return {'cpu_ram_mb': process.memory_info().rss / 1024**2}\n",
    "\n",
    "def compute_detailed_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    try:\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None, zero_division=0, labels=range(NUM_LABELS)\n",
    "        )\n",
    "    except ValueError:\n",
    "        precision = np.zeros(NUM_LABELS)\n",
    "        recall = np.zeros(NUM_LABELS)\n",
    "        f1 = np.zeros(NUM_LABELS)\n",
    "        support = np.bincount(y_true, minlength=NUM_LABELS)\n",
    "    \n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    minority_mask = np.isin(y_true, minority_ids)\n",
    "    if minority_mask.sum() > 0:\n",
    "        minority_true = y_true[minority_mask]\n",
    "        minority_pred = y_pred[minority_mask]\n",
    "        if len(np.unique(minority_true)) > 0 and len(minority_true) > 1:\n",
    "            try:\n",
    "                minority_f1 = f1_score(minority_true, minority_pred, average='macro', zero_division=0)\n",
    "            except:\n",
    "                minority_f1 = 0.0\n",
    "        else:\n",
    "            minority_f1 = 0.0\n",
    "    else:\n",
    "        minority_f1 = 0.0\n",
    "    \n",
    "    minority_f1_per_class = {}\n",
    "    for cls_id in minority_ids:\n",
    "        mask = (y_true == cls_id)\n",
    "        if mask.sum() > 1:\n",
    "            try:\n",
    "                minority_f1_per_class[id2label[cls_id]] = f1_score(\n",
    "                    y_true[mask], y_pred[mask], average='binary', zero_division=0\n",
    "                )\n",
    "            except:\n",
    "                minority_f1_per_class[id2label[cls_id]] = 0.0\n",
    "        else:\n",
    "            minority_f1_per_class[id2label[cls_id]] = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'weighted_f1': float(weighted_f1),\n",
    "        'macro_precision': float(np.mean(precision)),\n",
    "        'macro_recall': float(np.mean(recall)),\n",
    "        'minority_macro_f1': float(minority_f1),\n",
    "        'minority_f1_per_class': minority_f1_per_class,\n",
    "        'per_class_precision': {id2label[i]: float(precision[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_recall': {id2label[i]: float(recall[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_f1': {id2label[i]: float(f1[i]) for i in range(NUM_LABELS)},\n",
    "        'per_class_support': {id2label[i]: int(support[i]) for i in range(NUM_LABELS)},\n",
    "    }\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def load_jsonl(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                data.append(json.loads(s))\n",
    "    return data\n",
    "\n",
    "def extract_data(docs, max_sents=MAX_SENTS_PER_DOC):\n",
    "    all_sents, all_labels, doc_ids = [], [], []\n",
    "    for doc in docs:\n",
    "        doc_id = doc.get(\"id\", \"\")\n",
    "        sents, labs = [], []\n",
    "        if \"sentences\" in doc and \"labels\" in doc:\n",
    "            sents = doc[\"sentences\"]\n",
    "            labs = [label2id.get(l, label2id[\"NONE\"]) for l in doc[\"labels\"]]\n",
    "        elif \"sentences\" in doc and \"annotation\" in doc:\n",
    "            sents = doc[\"sentences\"]\n",
    "            labs = [label2id.get(l, label2id[\"NONE\"]) for l in doc[\"annotation\"]]\n",
    "        elif \"annotations\" in doc:\n",
    "            for a in doc.get(\"annotations\", []):\n",
    "                for item in a.get(\"result\", []):\n",
    "                    val = item.get(\"value\", {})\n",
    "                    text = val.get(\"text\", \"\").strip()\n",
    "                    labs_list = val.get(\"labels\", [\"NONE\"])\n",
    "                    if text:\n",
    "                        sents.append(text)\n",
    "                        labs.append(label2id.get(labs_list[0], label2id[\"NONE\"]))\n",
    "        if len(sents) > max_sents:\n",
    "            sents = sents[:max_sents]\n",
    "            labs = labs[:max_sents]\n",
    "        if sents and labs and len(sents) == len(labs):\n",
    "            all_sents.append(sents)\n",
    "            all_labels.append(labs)\n",
    "            doc_ids.append(doc_id)\n",
    "    return all_sents, all_labels, doc_ids\n",
    "\n",
    "# ---------------- PROTOTYPE MANAGER ----------------\n",
    "class ClassPrototypeManager:\n",
    "    def __init__(self):\n",
    "        self.prototypes = None\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, embeddings, labels):\n",
    "        embeddings = np.asarray(embeddings)\n",
    "        labels = np.asarray(labels)\n",
    "        D = embeddings.shape[1]\n",
    "        protos = np.zeros((NUM_LABELS, D), dtype=np.float32)\n",
    "        for k in range(NUM_LABELS):\n",
    "            mask = labels == k\n",
    "            if mask.sum() > 0:\n",
    "                protos[k] = embeddings[mask].mean(axis=0)\n",
    "            else:\n",
    "                protos[k] = np.random.randn(D).astype(np.float32) * 1e-3\n",
    "        self.prototypes = protos\n",
    "        self.fitted = True\n",
    "        print(f\"[Prototypes] fitted, shape {protos.shape}\")\n",
    "\n",
    "    def get_all_tensor(self, device=None):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        t = torch.tensor(self.prototypes, dtype=torch.float32)\n",
    "        if device is not None:\n",
    "            t = t.to(device)\n",
    "        return t\n",
    "\n",
    "    def get_nearest_index(self, embeddings):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        emb = np.asarray(embeddings)\n",
    "        emb_n = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-8)\n",
    "        proto_n = self.prototypes / (np.linalg.norm(self.prototypes, axis=1, keepdims=True) + 1e-8)\n",
    "        sims = emb_n @ proto_n.T\n",
    "        return np.argmax(sims, axis=1)\n",
    "\n",
    "    def knn_prior(self, embeddings, topk=KNN_K):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Prototypes not fitted\")\n",
    "        emb = np.asarray(embeddings)\n",
    "        emb_n = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
    "        proto_n = self.prototypes / (np.linalg.norm(self.prototypes, axis=1, keepdims=True) + 1e-12)\n",
    "        sims = emb_n @ proto_n.T\n",
    "        topk_idx = np.argsort(-sims, axis=1)[:, :topk]\n",
    "        topk_sims = np.take_along_axis(sims, topk_idx, axis=1)\n",
    "        return topk_sims, topk_idx\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class DocumentTextDataset(Dataset):\n",
    "    def __init__(self, docs_sents, docs_labels):\n",
    "        self.docs_sents = docs_sents\n",
    "        self.docs_labels = docs_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.docs_sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sentences\": self.docs_sents[idx],\n",
    "            \"labels\": torch.tensor(self.docs_labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def collate_docs(batch, tokenizer):\n",
    "    max_sents = max(len(b[\"sentences\"]) for b in batch)\n",
    "    B = len(batch)\n",
    "\n",
    "    flat_sents = []\n",
    "    doc_sent_offsets = []\n",
    "    for b in batch:\n",
    "        doc_sent_offsets.append(len(flat_sents))\n",
    "        flat_sents.extend(b[\"sentences\"])\n",
    "\n",
    "    enc = tokenizer(\n",
    "        flat_sents,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids_all = enc[\"input_ids\"]\n",
    "    attn_mask_all = enc[\"attention_mask\"]\n",
    "\n",
    "    max_tokens = input_ids_all.shape[1]\n",
    "    input_ids_padded = torch.zeros((B, max_sents, max_tokens), dtype=torch.long)\n",
    "    attn_mask_padded = torch.zeros((B, max_sents, max_tokens), dtype=torch.long)\n",
    "    labels_padded = torch.full((B, max_sents), -100, dtype=torch.long)\n",
    "    lengths = torch.zeros(B, dtype=torch.long)\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        num_s = len(b[\"sentences\"])\n",
    "        lengths[i] = num_s\n",
    "        start = doc_sent_offsets[i]\n",
    "        end = start + num_s\n",
    "        input_ids_padded[i, :num_s] = input_ids_all[start:end]\n",
    "        attn_mask_padded[i, :num_s] = attn_mask_all[start:end]\n",
    "        labels_padded[i, :num_s] = b[\"labels\"]\n",
    "\n",
    "    return input_ids_padded, attn_mask_padded, labels_padded, lengths\n",
    "\n",
    "# ---------------- FOCAL LOSS ----------------\n",
    "def focal_loss(logits_masked, labels_masked, gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA, label_smoothing=LABEL_SMOOTHING):\n",
    "    ce = F.cross_entropy(logits_masked, labels_masked, reduction='none', label_smoothing=label_smoothing)\n",
    "    pt = torch.exp(-ce)\n",
    "    focal = alpha * (1-pt)**gamma * ce\n",
    "    return focal.mean()\n",
    "\n",
    "# ---------------- MODEL COMPONENTS ----------------\n",
    "class RolePrototypicalLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.proto = nn.Parameter(torch.randn(NUM_LABELS, hidden_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        h_norm = F.normalize(h, dim=-1)\n",
    "        proto_norm = F.normalize(self.proto, dim=-1)\n",
    "        return h_norm @ proto_norm.T\n",
    "\n",
    "class RoleTransitionMatrix(nn.Module):\n",
    "    def __init__(self, rtm_lambda=RTM_LAMBDA):\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.zeros(NUM_LABELS, NUM_LABELS))\n",
    "        self.rtm_lambda = rtm_lambda\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        lp = logits.log_softmax(-1)\n",
    "        B, S, C = lp.shape\n",
    "        \n",
    "        for t in range(1, S):\n",
    "            tr = torch.logsumexp(\n",
    "                lp[:, t-1].unsqueeze(2) + self.A.log_softmax(-1),\n",
    "                dim=1\n",
    "            )\n",
    "            logits[:, t] += self.rtm_lambda * tr\n",
    "        return logits\n",
    "\n",
    "class SentenceEncoderFFN(nn.Module):\n",
    "    def __init__(self, sent_dim, hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(sent_dim, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, sent_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(sent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = self.act(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        return self.ln(x + h)\n",
    "\n",
    "class PrototypeAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, sent_emb, prototypes):\n",
    "        B, S, H = sent_emb.shape\n",
    "        h_proj = self.W(sent_emb)\n",
    "        scores = torch.matmul(h_proj, prototypes.t())\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        proto_ctx = torch.matmul(attn_weights, prototypes)\n",
    "        return proto_ctx, attn_weights\n",
    "\n",
    "def prototypical_cosine_loss(reprs, prototypes_tensor, labels, temperature=PROTO_AUX_TEMPERATURE):\n",
    "    x_norm = F.normalize(reprs, p=2, dim=1)\n",
    "    p_norm = F.normalize(prototypes_tensor, p=2, dim=1)\n",
    "    sims = x_norm @ p_norm.t()\n",
    "    sims = sims * temperature\n",
    "    loss = F.cross_entropy(sims, labels)\n",
    "    return loss, sims\n",
    "\n",
    "def compute_losses(logits, labels, sent_emb_flat, doc_out, prototypes_tensor, rpl_proto, temperature=PROTO_AUX_TEMPERATURE):\n",
    "    logits_flat = logits.view(-1, NUM_LABELS)\n",
    "    labels_flat = labels.view(-1)\n",
    "    mask = labels_flat != -100\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device), 0.0, 0.0\n",
    "    \n",
    "    logits_masked = logits_flat[mask]\n",
    "    labels_masked = labels_flat[mask]\n",
    "    \n",
    "    ce_loss = focal_loss(logits_masked, labels_masked)\n",
    "    \n",
    "    valid_sent_emb = sent_emb_flat[mask]\n",
    "    proto_loss, _ = prototypical_cosine_loss(valid_sent_emb, prototypes_tensor, labels_masked)\n",
    "    \n",
    "    valid_doc_out = doc_out.view(-1, doc_out.size(-1))[mask]\n",
    "    rpl_sim = F.normalize(valid_doc_out, dim=1) @ F.normalize(rpl_proto, dim=1).T\n",
    "    rpl_loss = F.cross_entropy(rpl_sim * temperature, labels_masked)\n",
    "    \n",
    "    return ce_loss, proto_loss, rpl_loss\n",
    "\n",
    "# ---------------- ‚úÖ FIXED MODEL (No enable_adapters) ----------------\n",
    "class ProtoHSLNModel(nn.Module):\n",
    "    def __init__(self, bert_name=INLEGALBERT_MODEL_NAME, pos_dim=POS_EMB_DIM,\n",
    "                use_pos_emb=USE_POSITIONAL_EMB, use_knn_prior=USE_KNN_PRIOR,\n",
    "                knn_prior_dim=KNN_PRIOR_DIM, doc_hidden=LSTM_HIDDEN, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "\n",
    "        base_model = AutoModel.from_pretrained(bert_name)\n",
    "        \n",
    "        # ‚úÖ FIXED: Standard AdaLoRA config - NO enable_adapters()\n",
    "        adalora_config = AdaLoraConfig(\n",
    "            r=ADALORA_INIT_R,\n",
    "            target_r=ADALORA_TARGET_R,\n",
    "            init_r=ADALORA_INIT_R,\n",
    "            tinit=ADALORA_TINIT,\n",
    "            tfinal=ADALORA_TFINAL,\n",
    "            deltaT=ADALORA_DELTA_T,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            # ‚úÖ Conservative target modules (works reliably)\n",
    "            target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "        )\n",
    "        self.bert = get_peft_model(base_model, adalora_config)\n",
    "\n",
    "        # ‚úÖ FIXED: Print trainable params FIRST (before any enable calls)\n",
    "        print(\"\\nüìä BERT Trainable Parameters:\")\n",
    "        self.bert.print_trainable_parameters()\n",
    "        \n",
    "        # ‚úÖ Enable gradients for ALL LoRA parameters safely\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if any(x in name for x in [\"lora_\", \"ranknum\"]):\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        self.use_pos_emb = use_pos_emb\n",
    "        if use_pos_emb and pos_dim > 0:\n",
    "            self.pos_emb = nn.Embedding(1024, pos_dim)\n",
    "            self.pos_proj = nn.Linear(pos_dim, self.hidden_size) if pos_dim != self.hidden_size else None\n",
    "        else:\n",
    "            self.pos_emb = None\n",
    "            self.pos_proj = None\n",
    "\n",
    "        self.use_knn_prior = use_knn_prior\n",
    "        self.knn_prior_dim = knn_prior_dim\n",
    "        if use_knn_prior and knn_prior_dim > 0:\n",
    "            self.knn_proj = nn.Sequential(\n",
    "                nn.Linear(KNN_K, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, knn_prior_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.knn_proj = None\n",
    "\n",
    "        self.sent_encoder = SentenceEncoderFFN(self.hidden_size, hidden=512, dropout=dropout)\n",
    "        self.proto_attn = PrototypeAttention(self.hidden_size)\n",
    "        \n",
    "        final_in_dim = self.hidden_size + (self.knn_prior_dim if self.knn_proj else 0)\n",
    "        self.doc_lstm = nn.LSTM(final_in_dim, doc_hidden, 2, bidirectional=True, \n",
    "                               batch_first=True, dropout=dropout)\n",
    "        \n",
    "        lstm_out_dim = doc_hidden * 2\n",
    "        \n",
    "        self.ce_classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout), nn.Linear(lstm_out_dim, doc_hidden),\n",
    "            nn.ReLU(), nn.Dropout(dropout), nn.Linear(doc_hidden, NUM_LABELS),\n",
    "        )\n",
    "        self.rpl = RolePrototypicalLayer(lstm_out_dim)\n",
    "        self.rtm = RoleTransitionMatrix()\n",
    "        self.head_alpha = nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    def encode_sentences(self, input_ids, attention_mask):\n",
    "        B, S, T = input_ids.shape\n",
    "        input_ids_flat = input_ids.view(B * S, T)\n",
    "        attn_flat = attention_mask.view(B * S, T)\n",
    "        outputs = self.bert(input_ids_flat, attention_mask=attn_flat)\n",
    "        sent_emb_flat = outputs.last_hidden_state.mean(dim=1)\n",
    "        sent_emb = sent_emb_flat.view(B, S, -1)\n",
    "        return sent_emb, sent_emb_flat\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, lengths, prototypes_tensor, proto_idx_batch=None, knn_sims=None):\n",
    "        B, S, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        sent_emb, sent_emb_flat = self.encode_sentences(input_ids, attention_mask)\n",
    "        sent_emb = self.sent_encoder(sent_emb)\n",
    "\n",
    "        if self.pos_emb is not None:\n",
    "            pos_idx = torch.arange(S, device=device).unsqueeze(0).expand(B, -1)\n",
    "            pos_vec = self.pos_emb(pos_idx)\n",
    "            if self.pos_proj is not None:\n",
    "                pos_vec = self.pos_proj(pos_vec)\n",
    "            sent_emb = sent_emb + pos_vec\n",
    "\n",
    "        protos = prototypes_tensor if isinstance(prototypes_tensor, torch.Tensor) else \\\n",
    "                torch.tensor(prototypes_tensor, device=device, dtype=torch.float32)\n",
    "        proto_ctx, _ = self.proto_attn(sent_emb, protos)\n",
    "        sent_emb = sent_emb + proto_ctx\n",
    "\n",
    "        if self.knn_proj is not None and knn_sims is not None:\n",
    "            knn_feat = self.knn_proj(knn_sims.view(-1, KNN_K)).view(B, S, -1)\n",
    "            doc_in = torch.cat([sent_emb, knn_feat], dim=2)\n",
    "        else:\n",
    "            doc_in = sent_emb\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(doc_in, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.doc_lstm(packed)\n",
    "        doc_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        ce_logits = self.ce_classifier(doc_out)\n",
    "        rpl_logits = self.rpl(doc_out)\n",
    "        alpha = torch.sigmoid(self.head_alpha)\n",
    "        blended_logits = alpha * ce_logits + (1 - alpha) * rpl_logits\n",
    "        final_logits = self.rtm(blended_logits)\n",
    "        \n",
    "        return final_logits, sent_emb_flat, doc_out\n",
    "\n",
    "# ---------------- ‚úÖ FIXED TRAINER ----------------\n",
    "class Trainer:\n",
    "    def __init__(self, model, tokenizer, prototype_manager, device=DEVICE):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prototype_manager = prototype_manager\n",
    "        self.device = device\n",
    "        self.global_step = 0\n",
    "\n",
    "    def _build_train_loader(self, train_dataset):\n",
    "        if not USE_WEIGHTED_SAMPLER:\n",
    "            return DataLoader(train_dataset, batch_size=BATCH_DOCS, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "        \n",
    "        major_labels = []\n",
    "        for doc_labels in train_dataset.docs_labels:\n",
    "            if doc_labels:\n",
    "                doc_counter = Counter(doc_labels)\n",
    "                major_label = doc_counter.most_common(1)[0][0]\n",
    "                major_labels.append(major_label)\n",
    "            else:\n",
    "                major_labels.append(0)\n",
    "        \n",
    "        counts = np.bincount(major_labels, minlength=NUM_LABELS)\n",
    "        inv_freq = 1.0 / (counts + 1e-6)\n",
    "        \n",
    "        weights = inv_freq[np.array(major_labels)].copy()\n",
    "        for doc_idx, major_label in enumerate(major_labels):\n",
    "            if major_label in minority_ids:\n",
    "                weights[doc_idx] *= MINORITY_BOOST\n",
    "        \n",
    "        sampler = WeightedRandomSampler(torch.tensor(weights, dtype=torch.double),\n",
    "                                       num_samples=len(weights), replacement=True)\n",
    "        return DataLoader(train_dataset, batch_size=BATCH_DOCS, sampler=sampler,\n",
    "                         collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "\n",
    "    def safe_adalora_update(self, global_step):\n",
    "        \"\"\"‚úÖ Ultra-safe AdaLoRA update - fails silently\"\"\"\n",
    "        try:\n",
    "            if (global_step >= ADALORA_TINIT and \n",
    "                global_step <= ADALORA_TFINAL and \n",
    "                global_step % ADALORA_DELTA_T == 0):\n",
    "                \n",
    "                if hasattr(self.model.bert, 'update_and_allocate'):\n",
    "                    self.model.bert.update_and_allocate(global_step)\n",
    "                    return True\n",
    "        except Exception:\n",
    "            pass  # Silent fail - training continues\n",
    "        return False\n",
    "\n",
    "    def train(self, train_dataset, dev_dataset, num_epochs=NUM_EPOCHS, lr=LR, \n",
    "              proto_weight=PROTO_WEIGHT, rpl_weight=RPL_WEIGHT):\n",
    "        start_time = time.time()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "        train_loader = self._build_train_loader(train_dataset)\n",
    "        total_steps = max(1, len(train_loader) * num_epochs)\n",
    "        warmup_steps = max(1, int(0.05 * total_steps))\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        \n",
    "        prototypes_tensor = self.prototype_manager.get_all_tensor(device=self.device)\n",
    "        best_macro_f1 = -1.0\n",
    "        best_ckpt = None\n",
    "        history = []\n",
    "        adalora_updates = 0\n",
    "\n",
    "        print(\"üöÄ Starting training with FIXED AdaLoRA + imbalance handling...\")\n",
    "        print(f\"‚úÖ AdaLoRA: init_r={ADALORA_INIT_R}, target_r={ADALORA_TARGET_R}\")\n",
    "        print(f\"‚úÖ Minority boost: {MINORITY_BOOST}x | Focal gamma: {FOCAL_GAMMA}\")\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            self.model.train()\n",
    "            t0 = time.time()\n",
    "            running_ce = running_proto = running_rpl = running_total = 0.0\n",
    "            n_samples = 0\n",
    "\n",
    "            for batch_idx, (input_ids, attn_mask, labels, lengths) in enumerate(train_loader):\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                attn_mask = attn_mask.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                lengths = lengths.to(self.device)\n",
    "\n",
    "                # ‚úÖ KNN prototype features\n",
    "                with torch.no_grad():\n",
    "                    sent_emb, sent_emb_flat_for_knn = self.model.encode_sentences(input_ids, attn_mask)\n",
    "                    sent_emb_flat_for_knn_np = sent_emb_flat_for_knn.view(-1, sent_emb_flat_for_knn.size(-1)).cpu().numpy()\n",
    "                    sims, _ = self.prototype_manager.knn_prior(sent_emb_flat_for_knn_np, topk=KNN_K)\n",
    "                    knn_sims_tensor = torch.tensor(sims, dtype=torch.float32, device=self.device)\n",
    "                    nearest_idx_flat = self.prototype_manager.get_nearest_index(sent_emb_flat_for_knn_np)\n",
    "                    proto_idx_batch = torch.tensor(nearest_idx_flat, dtype=torch.long, \n",
    "                                                 device=self.device).view(input_ids.shape[:2])\n",
    "\n",
    "                logits, sent_emb_flat, doc_out = self.model(\n",
    "                    input_ids, attn_mask, lengths, prototypes_tensor,\n",
    "                    proto_idx_batch, knn_sims_tensor\n",
    "                )\n",
    "\n",
    "                ce_loss, proto_loss, rpl_loss = compute_losses(\n",
    "                    logits, labels, sent_emb_flat, doc_out,\n",
    "                    prototypes_tensor, self.model.rpl.proto\n",
    "                )\n",
    "                total_loss = ce_loss + proto_weight * proto_loss + rpl_weight * rpl_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # ‚úÖ Safe AdaLoRA update\n",
    "                self.global_step += 1\n",
    "                if self.safe_adalora_update(self.global_step):\n",
    "                    adalora_updates += 1\n",
    "\n",
    "                mask = labels.view(-1) != -100\n",
    "                n = mask.sum().item()\n",
    "                running_ce += ce_loss.item() * n\n",
    "                running_proto += proto_loss.item() * n\n",
    "                running_rpl += rpl_loss.item() * n\n",
    "                running_total += total_loss.item() * n\n",
    "                n_samples += n\n",
    "\n",
    "            val_results = self.evaluate(dev_dataset, measure_time=True)\n",
    "            avg_ce = running_ce / max(1, n_samples)\n",
    "            avg_proto = running_proto / max(1, n_samples)\n",
    "            avg_rpl = running_rpl / max(1, n_samples)\n",
    "            avg_total = running_total / max(1, n_samples)\n",
    "\n",
    "            epoch_time = time.time() - t0\n",
    "            mem_usage = get_memory_usage(self.device)\n",
    "\n",
    "            history.append({\n",
    "                \"epoch\": epoch, \n",
    "                \"train_ce\": avg_ce, \"train_proto\": avg_proto, \n",
    "                \"train_rpl\": avg_rpl, \"train_total\": avg_total,\n",
    "                \"val_acc\": val_results[\"metrics\"][\"accuracy\"], \n",
    "                \"val_macro_f1\": val_results[\"metrics\"][\"macro_f1\"],\n",
    "                \"epoch_time_s\": epoch_time,\n",
    "                \"mem_usage\": mem_usage,\n",
    "                \"adalora_updates\": adalora_updates,\n",
    "                \"time\": time.time() - start_time,\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | train:{avg_total:.4f} | \"\n",
    "                  f\"val:{val_results['total_loss']:.4f} acc:{val_results['metrics']['accuracy']:.4f} \"\n",
    "                  f\"macroF1:{val_results['metrics']['macro_f1']:.4f} | \"\n",
    "                  f\"time:{epoch_time/60:.1f}m | mem:{mem_usage.get('allocated', 0):.1f}MB | \"\n",
    "                  f\"AdaLoRA updates: {adalora_updates}\")\n",
    "\n",
    "            if val_results[\"metrics\"][\"macro_f1\"] > best_macro_f1 + 1e-4:\n",
    "                best_macro_f1 = val_results[\"metrics\"][\"macro_f1\"]\n",
    "                best_ckpt_path = os.path.join(OUT_DIR, f\"best_epoch{epoch}_f1{best_macro_f1:.4f}.pt\")\n",
    "                torch.save({\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"macro_f1\": best_macro_f1,\n",
    "                }, best_ckpt_path)\n",
    "                best_ckpt = best_ckpt_path\n",
    "\n",
    "        total_train_time = time.time() - start_time\n",
    "        pd.DataFrame(history).to_csv(os.path.join(OUT_DIR, \"training_history.csv\"), index=False)\n",
    "        \n",
    "        print(f\"üèÜ Total Training Time: {total_train_time/60:.1f} minutes\")\n",
    "        print(f\"üèÜ AdaLoRA updates performed: {adalora_updates}\")\n",
    "        print(f\"üèÜ Best checkpoint: {best_ckpt}\")\n",
    "        return best_ckpt, total_train_time\n",
    "\n",
    "    def evaluate(self, dataset, measure_time=False):\n",
    "        start_time = time.time()\n",
    "        self.model.eval()\n",
    "        loader = DataLoader(dataset, batch_size=2, shuffle=False,\n",
    "                           collate_fn=lambda b: collate_docs(b, self.tokenizer))\n",
    "        prototypes_tensor = self.prototype_manager.get_all_tensor(device=self.device)\n",
    "        all_preds, all_trues = [], []\n",
    "        running_ce = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attn_mask, labels, lengths in loader:\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                attn_mask = attn_mask.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                lengths = lengths.to(self.device)\n",
    "\n",
    "                sent_emb, sent_emb_flat_for_knn = self.model.encode_sentences(input_ids, attn_mask)\n",
    "                sent_emb_flat_for_knn_np = sent_emb_flat_for_knn.view(-1, sent_emb_flat_for_knn.size(-1)).cpu().numpy()\n",
    "                sims, _ = self.prototype_manager.knn_prior(sent_emb_flat_for_knn_np, topk=KNN_K)\n",
    "                knn_sims_tensor = torch.tensor(sims, dtype=torch.float32, device=self.device)\n",
    "                nearest_idx_flat = self.prototype_manager.get_nearest_index(sent_emb_flat_for_knn_np)\n",
    "                proto_idx_batch = torch.tensor(nearest_idx_flat, dtype=torch.long, \n",
    "                                             device=self.device).view(input_ids.shape[:2])\n",
    "\n",
    "                logits, sent_emb_flat, doc_out = self.model(\n",
    "                    input_ids, attn_mask, lengths, prototypes_tensor,\n",
    "                    proto_idx_batch, knn_sims_tensor\n",
    "                )\n",
    "                \n",
    "                logits_flat = logits.view(-1, NUM_LABELS)\n",
    "                labels_flat = labels.view(-1)\n",
    "                mask = labels_flat != -100\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                    \n",
    "                logits_masked = logits_flat[mask]\n",
    "                labels_masked = labels_flat[mask]\n",
    "                \n",
    "                ce_loss = focal_loss(logits_masked, labels_masked)\n",
    "                preds = torch.argmax(logits_masked, dim=1).cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_trues.extend(labels_masked.cpu().numpy().tolist())\n",
    "                \n",
    "                n = labels_masked.size(0)\n",
    "                running_ce += ce_loss.item() * n\n",
    "                n_samples += n\n",
    "\n",
    "        if n_samples == 0:\n",
    "            return {\"metrics\": {\"accuracy\": 0, \"macro_f1\": 0}, \"total_loss\": 0, \"inference_time\": 0}\n",
    "\n",
    "        avg_ce = running_ce / n_samples\n",
    "        detailed_metrics = compute_detailed_metrics(all_trues, all_preds)\n",
    "        \n",
    "        cls_report = classification_report(\n",
    "            [id2label[x] for x in all_trues], [id2label[x] for x in all_preds],\n",
    "            digits=4, zero_division=0\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"metrics\": detailed_metrics,\n",
    "            \"total_loss\": avg_ce,\n",
    "            \"classification_report\": cls_report,\n",
    "            \"all_preds\": all_preds, \n",
    "            \"all_trues\": all_trues,\n",
    "            \"sample_count\": n_samples\n",
    "        }\n",
    "        \n",
    "        if measure_time:\n",
    "            result[\"inference_time\"] = time.time() - start_time\n",
    "            result[\"mem_usage\"] = get_memory_usage(self.device)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def main():\n",
    "    total_start_time = time.time()\n",
    "    print(\"üöÄ Loading data...\")\n",
    "    train_docs = load_jsonl(TRAIN_PATH)\n",
    "    dev_docs = load_jsonl(DEV_PATH)\n",
    "    test_docs = load_jsonl(TEST_PATH)\n",
    "    print(f\"Dataset sizes - Train: {len(train_docs)}, Dev: {len(dev_docs)}, Test: {len(test_docs)}\")\n",
    "\n",
    "    train_sents, train_labels, _ = extract_data(train_docs)\n",
    "    dev_sents, dev_labels, _ = extract_data(dev_docs)\n",
    "    test_sents, test_labels, _ = extract_data(test_docs)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(INLEGALBERT_MODEL_NAME)\n",
    "    temp_bert = AutoModel.from_pretrained(INLEGALBERT_MODEL_NAME).to(DEVICE)\n",
    "    temp_bert.eval()\n",
    "    \n",
    "    print(\"Computing train prototypes...\")\n",
    "    with torch.no_grad():\n",
    "        flat_train_sents = [s for doc in train_sents for s in doc]\n",
    "        flat_train_labels = np.array([l for doc in train_labels for l in doc], dtype=np.int64)\n",
    "        train_embs = []\n",
    "        for i in range(0, len(flat_train_sents), 32):\n",
    "            batch = flat_train_sents[i:i+32]\n",
    "            enc = tokenizer(batch, padding=True, truncation=True, \n",
    "                          max_length=MAX_SEQ_LENGTH, return_tensors=\"pt\").to(DEVICE)\n",
    "            out = temp_bert(**enc).last_hidden_state.mean(dim=1)\n",
    "            train_embs.append(out.cpu().numpy())\n",
    "        train_embs = np.vstack(train_embs)\n",
    "\n",
    "    proto_mgr = ClassPrototypeManager()\n",
    "    proto_mgr.fit(train_embs, flat_train_labels)\n",
    "\n",
    "    train_dataset = DocumentTextDataset(train_sents, train_labels)\n",
    "    dev_dataset = DocumentTextDataset(dev_sents, dev_labels)\n",
    "    test_dataset = DocumentTextDataset(test_sents, test_labels)\n",
    "\n",
    "    print(\"‚úÖ Initializing ProtoHSLNModel with FIXED AdaLoRA...\")\n",
    "    model = ProtoHSLNModel()\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"üìä Total params: {total_params:,} | Trainable: {trainable_params:,} \"\n",
    "          f\"({100.0 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "    trainer = Trainer(model, tokenizer, proto_mgr, device=DEVICE)\n",
    "    \n",
    "    best_ckpt, train_time = trainer.train(train_dataset, dev_dataset, \n",
    "                                        proto_weight=PROTO_WEIGHT, rpl_weight=RPL_WEIGHT)\n",
    "    \n",
    "    if best_ckpt:\n",
    "        ckpt = torch.load(best_ckpt, map_location=DEVICE)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        print(\"‚úÖ Loaded best model\")\n",
    "\n",
    "    print(\"\\nüîç FINAL TEST EVALUATION\")\n",
    "    test_start_time = time.time()\n",
    "    test_results = trainer.evaluate(test_dataset, measure_time=True)\n",
    "    test_inference_time = test_results.get(\"inference_time\", 0)\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Test Inference Time: {test_inference_time:.2f}s\")\n",
    "    print(f\"üìä Test Memory Usage: {test_results.get('mem_usage', {})}\")\n",
    "    print(f\"üìà Test Accuracy:     {test_results['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"üéØ Test Macro-F1:     {test_results['metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"üîç Minority Macro-F1: {test_results['metrics']['minority_macro_f1']:.4f}\")\n",
    "    print(\"\\nüìã Test Classification Report:\")\n",
    "    print(test_results[\"classification_report\"])\n",
    "    \n",
    "    print(\"\\nüéØ Minority Class F1 Scores:\")\n",
    "    for cls, f1 in sorted(test_results['metrics']['minority_f1_per_class'].items()):\n",
    "        print(f\"  {cls}: {f1:.4f}\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    results_summary = {\n",
    "        'total_train_time_minutes': train_time / 60,\n",
    "        'test_inference_time_seconds': test_inference_time,\n",
    "        **test_results['metrics']\n",
    "    }\n",
    "    pd.DataFrame([results_summary]).to_csv(os.path.join(OUT_DIR, \"final_results_summary.csv\"), index=False)\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        \"true\": [id2label[x] for x in test_results[\"all_trues\"]],\n",
    "        \"pred\": [id2label[x] for x in test_results[\"all_preds\"]],\n",
    "    })\n",
    "    pred_df.to_csv(os.path.join(OUT_DIR, \"final_test_preds.csv\"), index=False)\n",
    "    \n",
    "    total_runtime = time.time() - total_start_time\n",
    "    print(f\"\\nüíæ Saved predictions to {OUT_DIR}/final_test_preds.csv\")\n",
    "    print(f\"‚è±Ô∏è  Total Runtime: {total_runtime/60:.1f} minutes\")\n",
    "    print(f\"üèÜ FIXED AdaLoRA + Imbalance Handling Training COMPLETED SUCCESSFULLY! ‚úÖ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d094cf-97e3-4407-a8b9-720442e5e3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-nlp",
   "language": "python",
   "name": "legal-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
